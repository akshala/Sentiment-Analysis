{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/akshala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/akshala/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/akshala/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/akshala/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import VaderConstants\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader = SentimentIntensityAnalyzer()\n",
    "tk = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "    # x: raw input text\n",
    "    x = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', x)  # removing website URLs\n",
    "    x = re.sub(r'http\\S+', '', x)   \n",
    "    x = re.sub('@[^\\s]+', '', x)   # removing usernames\n",
    "    return x  # preprocessed text is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderScore(x):\n",
    "    # x: preprocessed text\n",
    "    sentiment_dict = vader.polarity_scores(x) # dictionary containing the vader scores\n",
    "    return list(sentiment_dict.values())  # list of vader sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bing_liu_lexicon():  # reading positive and negative words from bing liu lexicon\n",
    "    bing_liu = {}\n",
    "    file1 = open('bing_liu_positive.txt', 'r')  # positive word file\n",
    "    while True:\n",
    "        line = file1.readline() \n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        bing_liu[line] = 'pos'\n",
    "\n",
    "    file2 = open('bing_liu_negative.txt', 'r')  # negative word file\n",
    "    while True:\n",
    "        line = file2.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        bing_liu[line] = 'neg'\n",
    "    return bing_liu  # returns dictionary with words as keys and pos/neg as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpqa_lexicon():  # reading positive, negative and neutral words from mpqa lexicon\n",
    "    mpqa = {}\n",
    "    file1 = open('mpqa_pos_neg.tff', 'r')\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.strip()\n",
    "        line = line.split()\n",
    "        try:    \n",
    "            priorpolarity = line[5].split('=')[1]  # get the priorpolarity\n",
    "        except IndexError:\n",
    "            priorpolarity = line[6].split('=')[1]  \n",
    "        word = line[2].split('=')[1]\n",
    "        if priorpolarity == 'positive':\n",
    "            mpqa[word] = 'pos'\n",
    "        if priorpolarity == 'negative':\n",
    "            mpqa[word] = 'neg'\n",
    "        if priorpolarity == 'neutral':\n",
    "            mpqa[word] = 'neu'\n",
    "    return mpqa    # returns dictionary with words as keys and pos/neg/neu as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bing_liu_polar_word_count(x):\n",
    "    # x: preprocessed text\n",
    "    bing_liu = bing_liu_lexicon()  # dictionary with words as keys and pos/neg as value\n",
    "    x = tk.tokenize(x)   # get list of words by tokenizing x\n",
    "    positive_words = 0\n",
    "    negative_words = 0\n",
    "    for elt in x:\n",
    "        elt = elt.lower()  # lowercase words\n",
    "        try:\n",
    "            if bing_liu[elt] == 'pos':\n",
    "                positive_words += 1  # increment if positive word\n",
    "            else:\n",
    "                negative_words += 1  # increment if negative word\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return [positive_words, negative_words]  # returns list of pos/neg word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpqa_polar_word_count(x):\n",
    "    # x: preprocessed text\n",
    "    mpqa = bing_liu_lexicon()   # dictionary with words as keys and pos/neg/neu as value\n",
    "    x = tk.tokenize(x)          # get list of words by tokenizing x\n",
    "    positive_words = 0\n",
    "    negative_words = 0\n",
    "    neutral_words = 0\n",
    "    for elt in x:\n",
    "        elt = elt.lower()       # lowercase words\n",
    "        try:\n",
    "            if mpqa[elt] == 'pos':\n",
    "                positive_words += 1    # increment if positive word\n",
    "            elif mpqa[elt] == 'neg':\n",
    "                negative_words += 1    # increment if negative word\n",
    "            else:\n",
    "                neutral_words += 1     # increment if neutral word\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return [positive_words, negative_words, neutral_words]   # returns list of pos/neg/neu word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment140_lexicon():  # reading word and corresponding polarity score\n",
    "    file1 = open('sentiment140_unigram.txt', 'r')\n",
    "    sentiment_score = {}\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split()\n",
    "        line = list(filter(None, line))\n",
    "        sentiment_score[line[0]] = float(line[1])\n",
    "    return sentiment_score  # dictionary with words as keys and polarity score as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment140_polarity_score(x):\n",
    "    # x: preprocessed text\n",
    "    sentiment_score = sentiment140_lexicon()\n",
    "    score = [0, 0]\n",
    "    x = tk.tokenize(x)   # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        if elt in sentiment_score:\n",
    "            if sentiment_score[elt] > 0:  # if score is greater than 0 add to positive score\n",
    "                score[0] += sentiment_score[elt]\n",
    "            else:                         # else add to negative score\n",
    "                score[1] += sentiment_score[elt]\n",
    "    return score   # list containing positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afinn_polarity_score(x):\n",
    "    # x: preprocessed text\n",
    "    af = Afinn()\n",
    "    polarity_score = [0,0]\n",
    "    x = tk.tokenize(x)     # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        elt = elt.lower()  # lowercase words\n",
    "        aff_score = af.score(elt)  # get the AFINN score\n",
    "        if aff_score > 0:\n",
    "            polarity_score[0] += aff_score   # if score is greater than 0 add to positive score\n",
    "        else:\n",
    "            polarity_score[1] += aff_score   # else add to negative score\n",
    "    return polarity_score   # list containing positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiword_polarity_score(x):\n",
    "    # x: preprocessed text\n",
    "    score = [0,0]\n",
    "    lemmatizer = WordNetLemmatizer()           # lemmatizer\n",
    "    tagged_sentence = pos_tag(tk.tokenize(x))  # tokenize x and get pos tags  of words\n",
    "    for word, tag in tagged_sentence:\n",
    "        wn_tag = ''\n",
    "        if tag.startswith('N'):  # assign wordnet noun tag\n",
    "            wn_tag = wn.NOUN\n",
    "        elif tag.startswith('J'): # assign wordnet adjective tag\n",
    "            wn_tag = wn.ADJ\n",
    "        elif tag.startswith('R'): # assign wordnet adverb tag\n",
    "            wn_tag = wn.ADV\n",
    "        elif tag.startswith('V'): # assign wordnet verb tag\n",
    "            wn_tag = wn.VERB  \n",
    "        if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):  # if no pos tag then ignore word\n",
    "            continue     \n",
    "        lemma = lemmatizer.lemmatize(word, pos=wn_tag)  # if no lemma for the pos tag and word then ignore word\n",
    "        if not lemma:\n",
    "            continue\n",
    "        synsets = wn.synsets(lemma, pos=wn_tag)  # synsets\n",
    "        if not synsets:\n",
    "            continue\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())    \n",
    "        score[0] += swn_synset.pos_score()    # add to positive score\n",
    "        score[1] += swn_synset.neg_score()    # add to negative score\n",
    "    return score   # list containing positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NRC_hashtag_sentiment_lexicon():  # reading hashtags and polarity score\n",
    "    file1 = open('NRC_hashtag_sentiment_lexicon.txt', 'r')\n",
    "    hashtag = {}\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split()\n",
    "        line = list(filter(None, line))\n",
    "        hashtag[line[0]] = float(line[1])\n",
    "    return hashtag       # dictionary with words as keys and polarity score as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity_score_hashtag(x):\n",
    "    # x: preprocessed text\n",
    "    hashtag = NRC_hashtag_sentiment_lexicon()  # dictionary with words as keys and polarity score as value\n",
    "    score = [0, 0]\n",
    "    x = tk.tokenize(x)\n",
    "    for elt in x:\n",
    "        if elt in hashtag:\n",
    "            if hashtag[elt] > 0:           # if score is greater than 0 add to positive score\n",
    "                score[0] += hashtag[elt]\n",
    "            else:\n",
    "                score[1] += hashtag[elt]   # else add to negative score\n",
    "    return score                           # list containing positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NRC_emotion_lexicon():   # reading NRC emotion lexicon \n",
    "    file1 = open('NRC_word_emotion_lexicon.txt', 'r')\n",
    "    nrc_emotions = {}\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split()\n",
    "        line = list(filter(None, line))\n",
    "        word = line[0]\n",
    "        emotion = line[1]\n",
    "        presence = line[2]\n",
    "        if word not in nrc_emotions.keys():   # if emotion is present for the=at word append to list\n",
    "            nrc_emotions[word] = []\n",
    "        if presence == '1':\n",
    "            nrc_emotions[word].append(emotion)\n",
    "    return nrc_emotions     # dictionary with words as keys and list of emotions associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_word_count(x):   \n",
    "    # x: preprocessed text\n",
    "    nrc_emotions = NRC_emotion_lexicon()\n",
    "    emotions = {'anger':0, 'anticipation':1, 'disgust':2, 'fear':3, 'joy':4, 'negative':5, 'positive':6, 'sadness':7, 'surprise':8, 'trust':9}\n",
    "    emotion_vector = [0] * 10\n",
    "    x = tk.tokenize(x)     # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        elt = elt.lower()  # lowercase\n",
    "        if elt in nrc_emotions:\n",
    "            word_emotions = nrc_emotions[elt]     # add 1 to particular emotion if word corresponding to it comes\n",
    "            for emo in word_emotions:\n",
    "                emotion_vector[emotions[emo]] += 1\n",
    "    return emotion_vector       # count of words corresponding to each emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NRC10_expanded_lexicon():   # reading words and corresponding emotion score\n",
    "    file1 = open('NRC-10-expanded.csv', 'r')\n",
    "    expanded_emotions = {}\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        count += 1\n",
    "        if count == 1:    # ignore header row\n",
    "            continue\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split('\\t')\n",
    "        line = list(filter(None, line))\n",
    "        word = line[0]\n",
    "        emotion_scores = [float(elt.strip('\\n')) for elt in line[1:]]   # convert to float\n",
    "        expanded_emotions[word] = emotion_scores\n",
    "    return expanded_emotions   # dictionary with words as keys and emotion score as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_score(x):\n",
    "    # x: preprocessed text\n",
    "    expanded_emotions = NRC10_expanded_lexicon()\n",
    "    emotion_scores = [0] * 10\n",
    "    x = tk.tokenize(x)   # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        elt = elt.lower()   # lowercase\n",
    "        if elt in expanded_emotions:\n",
    "            emotion_scores = np.add(emotion_scores, expanded_emotions[elt])   # add emotion score of word to corresponding element in score list\n",
    "    return emotion_scores   # return list with aggregate emotion scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NRC_hashtag_emotion_lexicon():   # reading hashtag and corresponding emotion score\n",
    "    file1 = open('NRC_hashtag_emotion_lexicon.txt', 'r')\n",
    "    hashtag = {}\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split()\n",
    "        line = list(filter(None, line))\n",
    "        if line[1] not in hashtag:\n",
    "            hashtag[line[1]] = []\n",
    "        hashtag[line[1]].append((line[0], float(line[2])))\n",
    "    return hashtag     # dictionary with word as key and list of tuples with emotion and score as values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_score_hashtag(x):\n",
    "    hashtag = NRC_hashtag_emotion_lexicon()\n",
    "    emotions = {'anger':0, 'fear':1, 'anticipation':2, 'trust':3, 'surprise':4, 'sadness':5, 'joy':6, 'disgust':7}\n",
    "    emotion_vector = [0] * 8\n",
    "    x = tk.tokenize(x)      # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        elt = elt.lower()   # lowercase\n",
    "        if elt in hashtag:\n",
    "            hashtag_emotions = hashtag[elt]\n",
    "            for pair in hashtag_emotions:\n",
    "                emotion_vector[emotions[pair[0]]] += pair[1]  # add word score to corresponding emotion total score\n",
    "    return emotion_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affin_emoticon():   # read emoticon and corresponding score\n",
    "    file1 = open('affin_emoticon.txt', 'r')\n",
    "    emoticon = {}\n",
    "    while True:\n",
    "        line = file1.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split()\n",
    "        line = list(filter(None, line))\n",
    "        emoticon[line[0]] = int(line[1])\n",
    "    return emoticon    # dictionary with emoticon as key and score as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoticon_score(x):\n",
    "    # x: preprocessed text\n",
    "    emoticon = affin_emoticon()\n",
    "    score = [0, 0]\n",
    "    x = tk.tokenize(x)  # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        if elt in emoticon:\n",
    "            if emoticon[elt] > 0:          # if score is greater than 0 add to positive score\n",
    "                score[0] += emoticon[elt]\n",
    "            else:                          # else add to negative score\n",
    "                score[1] += emoticon[elt]   \n",
    "    return score           # list containing positive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negation_count(x):\n",
    "    # x: preprocessed text\n",
    "    count = 0\n",
    "    vd = VaderConstants\n",
    "    x = tk.tokenize(x)    # get list of words by tokenizing x\n",
    "    for elt in x:\n",
    "        elt = elt.lower()  # lowercase\n",
    "        if vd.negated(vd, [elt]):\n",
    "            count += 1\n",
    "    return count  # number of negation words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramFeatures(preprocessed, curr, n, min_df):\n",
    "    #preprocessed: list of tweets\n",
    "    #n is n in ngram\n",
    "    #curr: list of tweets that we want the ngram feature for\n",
    "    #min_df: threshold value\n",
    "    count_vect = CountVectorizer(ngram_range=(n,n), tokenizer=tk.tokenize, min_df=min_df)\n",
    "    count_vect.fit(preprocessed)\n",
    "    feature_mat = count_vect.transform(curr).toarray()\n",
    "    return feature_mat   # feature matrix for ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(df):  # apply above functions to generate columns in pandas dataframe\n",
    "    # df: input dataframe with text\n",
    "    df['Text'] = df['Text'].apply(lambda x: preprocess(x))\n",
    "\n",
    "    df['vader'] = df['Text'].apply(lambda x: vaderScore(x))\n",
    "\n",
    "    df['bing_liu'] = df['Text'].apply(lambda x: bing_liu_polar_word_count(x))\n",
    "\n",
    "    df['mpqa'] = df['Text'].apply(lambda x: mpqa_polar_word_count(x))\n",
    "\n",
    "    df['sentiment140_polarity_score'] = df['Text'].apply(lambda x: sentiment140_polarity_score(x))\n",
    "    df['afinn_polarity_score'] = df['Text'].apply(lambda x: afinn_polarity_score(x))\n",
    "    df['sentiword_polarity_score'] = df['Text'].apply(lambda x: sentiword_polarity_score(x))\n",
    "\n",
    "    df['polarity_score_hashtag'] = df['Text'].apply(lambda x: polarity_score_hashtag(x))\n",
    "\n",
    "    df['emotion_count'] = df['Text'].apply(lambda x: emotion_word_count(x))\n",
    "\n",
    "    df['emotion_score'] = df['Text'].apply(lambda x: emotion_score(x))\n",
    "\n",
    "    df['emotion_score_hashtag'] = df['Text'].apply(lambda x: emotion_score_hashtag(x))\n",
    "\n",
    "    df['emoticon_score'] = df['Text'].apply(lambda x: emoticon_score(x))\n",
    "\n",
    "    df['negation_count'] = df['Text'].apply(lambda x: negation_count(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('anger_train.tsv', sep='\\t')\n",
    "lexicon_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('anger_test.tsv', sep='\\t')\n",
    "lexicon_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = df_train['Text'].to_list()   \n",
    "test_preprocessed = df_test['Text'].to_list()\n",
    "preprocessed = train_preprocessed + test_preprocessed   # total text preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshala/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "# adding unigram and bigram features\n",
    "df_train = df_train.assign(unigram=[*ngramFeatures(preprocessed, train_preprocessed, 1, 5)])\n",
    "df_train = df_train.assign(bigram=[*ngramFeatures(preprocessed, train_preprocessed, 2, 2)])\n",
    "df_test = df_test.assign(unigram=[*ngramFeatures(preprocessed, test_preprocessed, 1, 5)])\n",
    "df_test = df_test.assign(bigram=[*ngramFeatures(preprocessed, test_preprocessed, 2, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('features_anger_train.csv')\n",
    "df_test.to_csv('features_anger_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(df_train['intensity'])\n",
    "y_train = [float(x) for x in y_train]\n",
    "\n",
    "y_test = list(df_test['intensity'])\n",
    "y_test = [float(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = df_train.drop(['S.No', 'Text', 'Emotion', 'intensity'], axis=1)\n",
    "test_features = df_test.drop(['S.No', 'Text', 'Emotion', 'intensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x(feature_dict):  # get a single list from dictionary containing int, float and list\n",
    "    # feature_dict: input dictionary with features\n",
    "    features = []\n",
    "    for key, val in feature_dict.items():\n",
    "        if isinstance(val, np.ndarray):\n",
    "            val = list(val)\n",
    "            val = [float(x) for x in val]  # convert to float\n",
    "            features.extend(val)\n",
    "        elif isinstance(val, list):\n",
    "            features.extend(val)\n",
    "        else:\n",
    "            features.append(val)\n",
    "    features = np.array(features, dtype=float)\n",
    "    return features   # final feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['x_train'] = train_features.apply(lambda row: get_x(row), axis=1)\n",
    "test_features['x_test'] = test_features.apply(lambda row: get_x(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features['x_train']\n",
    "X_train = [list(x) for x in X_train]\n",
    "\n",
    "X_test = test_features['x_test']\n",
    "X_test = [list(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)  \n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('anger_DecisionTree.sav', 'wb'))   # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('anger_pred_DT.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='rbf', C=1)\n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('anger_SVM.sav', 'wb'))   # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('anger_pred_SVM.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regressor = MLPRegressor(random_state=0, max_iter=500)\n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('anger_MLP.sav', 'wb'))   # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('anger_pred_MLP.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('joy_train.tsv', sep='\\t')\n",
    "lexicon_features(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('joy_test.tsv', sep='\\t')\n",
    "lexicon_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessed = df_train['Text'].to_list()\n",
    "test_preprocessed = df_test['Text'].to_list()\n",
    "preprocessed = train_preprocessed + test_preprocessed   # total text preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshala/.local/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.assign(unigram=[*ngramFeatures(preprocessed, train_preprocessed, 1, 5)])\n",
    "df_train = df_train.assign(bigram=[*ngramFeatures(preprocessed, train_preprocessed, 2, 2)])\n",
    "df_test = df_test.assign(unigram=[*ngramFeatures(preprocessed, test_preprocessed, 1, 5)])\n",
    "df_test = df_test.assign(bigram=[*ngramFeatures(preprocessed, test_preprocessed, 2, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('features_joy_train.csv')\n",
    "df_test.to_csv('features_joy_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(df_train['intensity'])\n",
    "y_train = [float(x) for x in y_train]\n",
    "\n",
    "y_test = list(df_test['intensity'])\n",
    "y_test = [float(x) for x in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = df_train.drop(['S.No', 'Text', 'Emotion', 'intensity'], axis=1)\n",
    "test_features = df_test.drop(['S.No', 'Text', 'Emotion', 'intensity'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features['x_train'] = train_features.apply(lambda row: get_x(row), axis=1)\n",
    "test_features['x_test'] = test_features.apply(lambda row: get_x(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features['x_train']\n",
    "X_train = [list(x) for x in X_train]\n",
    "\n",
    "X_test = test_features['x_test']\n",
    "X_test = [list(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "regressor = DecisionTreeRegressor(random_state = 0)  \n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('joy_DecisionTree.sav', 'wb'))   # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('joy_pred_DT.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor = SVR(kernel='rbf', C=1)\n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('joy_SVM.sav', 'wb'))    # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('joy_pred_SVM.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "regressor = MLPRegressor(random_state=0, max_iter=500)\n",
    "regressor.fit(X_train, y_train)\n",
    "pickle.dump(regressor, open('joy_MLP.sav', 'wb'))     # saving model using pickle\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred.tofile('joy_pred_MLP.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
